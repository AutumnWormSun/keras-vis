{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Keras Visualization Toolkit keras-vis is a high-level toolkit for visualizing and debugging your trained keras neural net models. Currently supported visualizations include: Activation maximization Saliency maps Class activation maps All visualizations by default support N-dimensional image inputs. i.e., it generalizes to N-dim image inputs to your model. The toolkit generalizes all of the above as energy minimization problems with a clean, easy to use, and extendable interface. Compatible with both theano and tensorflow backends with 'channels_first', 'channels_last' data format. Quick links Read the documentation at https://raghakot.github.io/keras-vis . The Japanese edition is https://keisen.github.io/keras-vis-docs-ja . Join the slack channel for questions/discussions. We are tracking new features/tasks in waffle.io . Would love it if you lend us a hand and submit PRs. Getting Started In image backprop problems, the goal is to generate an input image that minimizes some loss function. Setting up an image backprop problem is easy. Define weighted loss function Various useful loss functions are defined in losses . A custom loss function can be defined by implementing Loss.build_loss . from vis.losses import ActivationMaximization from vis.regularizers import TotalVariation, LPNorm filter_indices = [1, 2, 3] # Tuple consists of (loss_function, weight) # Add regularizers as needed. losses = [ (ActivationMaximization(keras_layer, filter_indices), 1), (LPNorm(model.input), 10), (TotalVariation(model.input), 10) ] Configure optimizer to minimize weighted loss In order to generate natural looking images, image search space is constrained using regularization penalties. Some common regularizers are defined in regularizers . Like loss functions, custom regularizer can be defined by implementing Loss.build_loss . from vis.optimizer import Optimizer optimizer = Optimizer(model.input, losses) opt_img, grads, _ = optimizer.minimize() Concrete examples of various supported visualizations can be found in examples folder . Installation 1) Install keras with theano or tensorflow backend. Note that this library requires Keras > 2.0 2) Install keras-vis From sources sudo python setup.py install PyPI package sudo pip install keras-vis Visualizations NOTE: The links are currently broken and the entire documentation is being reworked. Please see examples/ for samples. Neural nets are black boxes. In the recent years, several approaches for understanding and visualizing Convolutional Networks have been developed in the literature. They give us a way to peer into the black boxes, diagnose mis-classifications, and assess whether the network is over/under fitting. Guided backprop can also be used to create trippy art , neural/texture style transfer among the list of other growing applications. Various visualizations, documented in their own pages, are summarized here. Conv filter visualization Convolutional filters learn 'template matching' filters that maximize the output when a similar template pattern is found in the input image. Visualize those templates via Activation Maximization. Dense layer visualization How can we assess whether a network is over/under fitting or generalizing well? Attention Maps How can we assess whether a network is attending to correct parts of the image in order to generate a decision? Generating animated gif of optimization progress It is possible to generate an animated gif of optimization progress by leveraging callbacks . Following example shows how to visualize the activation maximization for 'ouzel' class (output_index: 20). from vis.losses import ActivationMaximization from vis.regularizers import TotalVariation, LPNorm from vis.modifiers import Jitter from vis.optimizer import Optimizer from vis.callbacks import GifGenerator from vis.utils.vggnet import VGG16 # Build the VGG16 network with ImageNet weights model = VGG16(weights='imagenet', include_top=True) print('Model loaded.') # The name of the layer we want to visualize # (see model definition in vggnet.py) layer_name = 'predictions' layer_dict = dict([(layer.name, layer) for layer in model.layers[1:]]) output_class = [20] losses = [ (ActivationMaximization(layer_dict[layer_name], output_class), 2), (LPNorm(model.input), 10), (TotalVariation(model.input), 10) ] opt = Optimizer(model.input, losses) opt.minimize(max_iter=500, verbose=True, image_modifiers=[Jitter()], callbacks=[GifGenerator('opt_progress')]) Notice how the output jitters around? This is because we used Jitter , a kind of ImageModifier that is known to produce crisper activation maximization images. As an exercise, try: Without Jitter Varying various loss weights Citation Please cite keras-vis in your publications if it helped your research. Here is an example BibTeX entry: @misc{raghakotkerasvis, title={keras-vis}, author={Kotikalapudi, Raghavendra and contributors}, year={2017}, publisher={GitHub}, howpublished={\\url{https://github.com/raghakot/keras-vis}}, }","title":"Home"},{"location":"#keras-visualization-toolkit","text":"keras-vis is a high-level toolkit for visualizing and debugging your trained keras neural net models. Currently supported visualizations include: Activation maximization Saliency maps Class activation maps All visualizations by default support N-dimensional image inputs. i.e., it generalizes to N-dim image inputs to your model. The toolkit generalizes all of the above as energy minimization problems with a clean, easy to use, and extendable interface. Compatible with both theano and tensorflow backends with 'channels_first', 'channels_last' data format.","title":"Keras Visualization Toolkit"},{"location":"#quick-links","text":"Read the documentation at https://raghakot.github.io/keras-vis . The Japanese edition is https://keisen.github.io/keras-vis-docs-ja . Join the slack channel for questions/discussions. We are tracking new features/tasks in waffle.io . Would love it if you lend us a hand and submit PRs.","title":"Quick links"},{"location":"#getting-started","text":"In image backprop problems, the goal is to generate an input image that minimizes some loss function. Setting up an image backprop problem is easy. Define weighted loss function Various useful loss functions are defined in losses . A custom loss function can be defined by implementing Loss.build_loss . from vis.losses import ActivationMaximization from vis.regularizers import TotalVariation, LPNorm filter_indices = [1, 2, 3] # Tuple consists of (loss_function, weight) # Add regularizers as needed. losses = [ (ActivationMaximization(keras_layer, filter_indices), 1), (LPNorm(model.input), 10), (TotalVariation(model.input), 10) ] Configure optimizer to minimize weighted loss In order to generate natural looking images, image search space is constrained using regularization penalties. Some common regularizers are defined in regularizers . Like loss functions, custom regularizer can be defined by implementing Loss.build_loss . from vis.optimizer import Optimizer optimizer = Optimizer(model.input, losses) opt_img, grads, _ = optimizer.minimize() Concrete examples of various supported visualizations can be found in examples folder .","title":"Getting Started"},{"location":"#installation","text":"1) Install keras with theano or tensorflow backend. Note that this library requires Keras > 2.0 2) Install keras-vis From sources sudo python setup.py install PyPI package sudo pip install keras-vis","title":"Installation"},{"location":"#visualizations","text":"NOTE: The links are currently broken and the entire documentation is being reworked. Please see examples/ for samples. Neural nets are black boxes. In the recent years, several approaches for understanding and visualizing Convolutional Networks have been developed in the literature. They give us a way to peer into the black boxes, diagnose mis-classifications, and assess whether the network is over/under fitting. Guided backprop can also be used to create trippy art , neural/texture style transfer among the list of other growing applications. Various visualizations, documented in their own pages, are summarized here.","title":"Visualizations"},{"location":"#conv-filter-visualization","text":"Convolutional filters learn 'template matching' filters that maximize the output when a similar template pattern is found in the input image. Visualize those templates via Activation Maximization.","title":"Conv filter visualization"},{"location":"#dense-layer-visualization","text":"How can we assess whether a network is over/under fitting or generalizing well?","title":"Dense layer visualization"},{"location":"#attention-maps","text":"How can we assess whether a network is attending to correct parts of the image in order to generate a decision?","title":"Attention Maps"},{"location":"#generating-animated-gif-of-optimization-progress","text":"It is possible to generate an animated gif of optimization progress by leveraging callbacks . Following example shows how to visualize the activation maximization for 'ouzel' class (output_index: 20). from vis.losses import ActivationMaximization from vis.regularizers import TotalVariation, LPNorm from vis.modifiers import Jitter from vis.optimizer import Optimizer from vis.callbacks import GifGenerator from vis.utils.vggnet import VGG16 # Build the VGG16 network with ImageNet weights model = VGG16(weights='imagenet', include_top=True) print('Model loaded.') # The name of the layer we want to visualize # (see model definition in vggnet.py) layer_name = 'predictions' layer_dict = dict([(layer.name, layer) for layer in model.layers[1:]]) output_class = [20] losses = [ (ActivationMaximization(layer_dict[layer_name], output_class), 2), (LPNorm(model.input), 10), (TotalVariation(model.input), 10) ] opt = Optimizer(model.input, losses) opt.minimize(max_iter=500, verbose=True, image_modifiers=[Jitter()], callbacks=[GifGenerator('opt_progress')]) Notice how the output jitters around? This is because we used Jitter , a kind of ImageModifier that is known to produce crisper activation maximization images. As an exercise, try: Without Jitter Varying various loss weights","title":"Generating animated gif of optimization progress"},{"location":"#citation","text":"Please cite keras-vis in your publications if it helped your research. Here is an example BibTeX entry: @misc{raghakotkerasvis, title={keras-vis}, author={Kotikalapudi, Raghavendra and contributors}, year={2017}, publisher={GitHub}, howpublished={\\url{https://github.com/raghakot/keras-vis}}, }","title":"Citation"},{"location":"vis.backend/","text":"Source: vis/backend/ init .py#L0 Global Variables name modify_model_backprop modify_model_backprop(model, backprop_modifier) Creates a copy of model by modifying all activations to use a custom op to modify the backprop behavior. Args: model : The keras.models.Model instance. backprop_modifier : One of {'guided', 'rectified'} Returns: A copy of model with modified activations for backwards pass. set_random_seed set_random_seed(seed_value=1337) Sets random seed value for reproducibility. Args: seed_value : The seed value to use. (Default Value = infamous 1337)","title":"backend"},{"location":"vis.backend/#global-variables","text":"name","title":"Global Variables"},{"location":"vis.backend/#modify_model_backprop","text":"modify_model_backprop(model, backprop_modifier) Creates a copy of model by modifying all activations to use a custom op to modify the backprop behavior. Args: model : The keras.models.Model instance. backprop_modifier : One of {'guided', 'rectified'} Returns: A copy of model with modified activations for backwards pass.","title":"modify_model_backprop"},{"location":"vis.backend/#set_random_seed","text":"set_random_seed(seed_value=1337) Sets random seed value for reproducibility. Args: seed_value : The seed value to use. (Default Value = infamous 1337)","title":"set_random_seed"},{"location":"vis.backprop_modifiers/","text":"Source: vis/backprop_modifiers.py#L0 guided guided(model) Modifies backprop to only propagate positive gradients for positive activations. Args: model : The keras.models.Model instance whose gradient computation needs to be overridden. References: Details on guided back propagation can be found in paper: [String For Simplicity: The All Convolutional Net] (https://arxiv.org/pdf/1412.6806.pdf) deconv, rectified, relu deconv, rectified, relu(model) Modifies backprop to only propagate positive gradients. Args: model : The keras.models.Model instance whose gradient computation needs to be overridden. References: Details can be found in the paper: [Visualizing and Understanding Convolutional Networks] (https://arxiv.org/pdf/1311.2901.pdf) get get(identifier)","title":"backprop_modifiers"},{"location":"vis.backprop_modifiers/#guided","text":"guided(model) Modifies backprop to only propagate positive gradients for positive activations. Args: model : The keras.models.Model instance whose gradient computation needs to be overridden. References: Details on guided back propagation can be found in paper: [String For Simplicity: The All Convolutional Net] (https://arxiv.org/pdf/1412.6806.pdf)","title":"guided"},{"location":"vis.backprop_modifiers/#deconv-rectified-relu","text":"deconv, rectified, relu(model) Modifies backprop to only propagate positive gradients. Args: model : The keras.models.Model instance whose gradient computation needs to be overridden. References: Details can be found in the paper: [Visualizing and Understanding Convolutional Networks] (https://arxiv.org/pdf/1311.2901.pdf)","title":"deconv, rectified, relu"},{"location":"vis.backprop_modifiers/#get","text":"get(identifier)","title":"get"},{"location":"vis.callbacks/","text":"Source: vis/callbacks.py#L0 OptimizerCallback Abstract class for defining callbacks for use with Optimizer.minimize . OptimizerCallback.callback callback(self, i, named_losses, overall_loss, grads, wrt_value) This function will be called within optimizer.minimize . Args: i : The optimizer iteration. named_losses : List of (loss_name, loss_value) tuples. overall_loss : Overall weighted loss. grads : The gradient of input image with respect to wrt_value . wrt_value : The current wrt_value . OptimizerCallback.on_end on_end(self) Called at the end of optimization process. This function is typically used to cleanup / close any opened resources at the end of optimization. Print Callback to print values during optimization. Print.callback callback(self, i, named_losses, overall_loss, grads, wrt_value) Print.on_end on_end(self) Called at the end of optimization process. This function is typically used to cleanup / close any opened resources at the end of optimization. GifGenerator Callback to construct gif of optimized image. GifGenerator. __init__ __init__(self, path) Args: path : The file path to save gif. GifGenerator.callback callback(self, i, named_losses, overall_loss, grads, wrt_value) GifGenerator.on_end on_end(self)","title":"callbacks"},{"location":"vis.callbacks/#optimizercallback","text":"Abstract class for defining callbacks for use with Optimizer.minimize .","title":"OptimizerCallback"},{"location":"vis.callbacks/#optimizercallbackcallback","text":"callback(self, i, named_losses, overall_loss, grads, wrt_value) This function will be called within optimizer.minimize . Args: i : The optimizer iteration. named_losses : List of (loss_name, loss_value) tuples. overall_loss : Overall weighted loss. grads : The gradient of input image with respect to wrt_value . wrt_value : The current wrt_value .","title":"OptimizerCallback.callback"},{"location":"vis.callbacks/#optimizercallbackon_end","text":"on_end(self) Called at the end of optimization process. This function is typically used to cleanup / close any opened resources at the end of optimization.","title":"OptimizerCallback.on_end"},{"location":"vis.callbacks/#print","text":"Callback to print values during optimization.","title":"Print"},{"location":"vis.callbacks/#printcallback","text":"callback(self, i, named_losses, overall_loss, grads, wrt_value)","title":"Print.callback"},{"location":"vis.callbacks/#printon_end","text":"on_end(self) Called at the end of optimization process. This function is typically used to cleanup / close any opened resources at the end of optimization.","title":"Print.on_end"},{"location":"vis.callbacks/#gifgenerator","text":"Callback to construct gif of optimized image.","title":"GifGenerator"},{"location":"vis.callbacks/#gifgenerator__init__","text":"__init__(self, path) Args: path : The file path to save gif.","title":"GifGenerator.__init__"},{"location":"vis.callbacks/#gifgeneratorcallback","text":"callback(self, i, named_losses, overall_loss, grads, wrt_value)","title":"GifGenerator.callback"},{"location":"vis.callbacks/#gifgeneratoron_end","text":"on_end(self)","title":"GifGenerator.on_end"},{"location":"vis.grad_modifiers/","text":"Source: vis/grad_modifiers.py#L0 negate negate(grads) Negates the gradients. Args: grads : A numpy array of grads to use. Returns: The negated gradients. absolute absolute(grads) Computes absolute gradients. Args: grads : A numpy array of grads to use. Returns: The absolute gradients. invert invert(grads) Inverts the gradients. Args: grads : A numpy array of grads to use. Returns: The inverted gradients. relu relu(grads) Clips negative gradient values. Args: grads : A numpy array of grads to use. Returns: The rectified gradients. small_values small_values(grads) Can be used to highlight small gradient values. Args: grads : A numpy array of grads to use. Returns: The modified gradients that highlight small values. get get(identifier)","title":"grad_modifiers"},{"location":"vis.grad_modifiers/#negate","text":"negate(grads) Negates the gradients. Args: grads : A numpy array of grads to use. Returns: The negated gradients.","title":"negate"},{"location":"vis.grad_modifiers/#absolute","text":"absolute(grads) Computes absolute gradients. Args: grads : A numpy array of grads to use. Returns: The absolute gradients.","title":"absolute"},{"location":"vis.grad_modifiers/#invert","text":"invert(grads) Inverts the gradients. Args: grads : A numpy array of grads to use. Returns: The inverted gradients.","title":"invert"},{"location":"vis.grad_modifiers/#relu","text":"relu(grads) Clips negative gradient values. Args: grads : A numpy array of grads to use. Returns: The rectified gradients.","title":"relu"},{"location":"vis.grad_modifiers/#small_values","text":"small_values(grads) Can be used to highlight small gradient values. Args: grads : A numpy array of grads to use. Returns: The modified gradients that highlight small values.","title":"small_values"},{"location":"vis.grad_modifiers/#get","text":"get(identifier)","title":"get"},{"location":"vis.input_modifiers/","text":"Source: vis/input_modifiers.py#L0 InputModifier Abstract class for defining an input modifier. An input modifier can be used with the Optimizer.minimize to make pre and post changes to the optimized input during the optimization process. modifier.pre(seed_input) # gradient descent update to img modifier.post(seed_input) InputModifier.post post(self, inp) Implement post gradient descent update modification to the input. If post-processing is not desired, simply ignore the implementation. It returns the unmodified inp by default. Args: inp : An N-dim numpy array of shape: (samples, channels, image_dims...) if image_data_format= channels_first or (samples, image_dims..., channels) if image_data_format=channels_last . Returns: The modified post input. InputModifier.pre pre(self, inp) Implement pre gradient descent update modification to the input. If pre-processing is not desired, simply ignore the implementation. It returns the unmodified inp by default. Args: inp : An N-dim numpy array of shape: (samples, channels, image_dims...) if image_data_format= channels_first or (samples, image_dims..., channels) if image_data_format=channels_last . Returns: The modified pre input. Jitter Jitter. __init__ __init__(self, jitter=0.05) Implements an input modifier that introduces random jitter in pre . Jitter has been shown to produce crisper activation maximization images. Args: jitter : The amount of jitter to apply, scalar or sequence. If a scalar, same jitter is applied to all image dims. If sequence, jitter should contain a value per image dim. A value between [0., 1.] is interpreted as a percentage of the image dimension. (Default value: 0.05) Jitter.post post(self, inp) Implement post gradient descent update modification to the input. If post-processing is not desired, simply ignore the implementation. It returns the unmodified inp by default. Args: inp : An N-dim numpy array of shape: (samples, channels, image_dims...) if image_data_format= channels_first or (samples, image_dims..., channels) if image_data_format=channels_last . Returns: The modified post input. Jitter.pre pre(self, img)","title":"input_modifiers"},{"location":"vis.input_modifiers/#inputmodifier","text":"Abstract class for defining an input modifier. An input modifier can be used with the Optimizer.minimize to make pre and post changes to the optimized input during the optimization process. modifier.pre(seed_input) # gradient descent update to img modifier.post(seed_input)","title":"InputModifier"},{"location":"vis.input_modifiers/#inputmodifierpost","text":"post(self, inp) Implement post gradient descent update modification to the input. If post-processing is not desired, simply ignore the implementation. It returns the unmodified inp by default. Args: inp : An N-dim numpy array of shape: (samples, channels, image_dims...) if image_data_format= channels_first or (samples, image_dims..., channels) if image_data_format=channels_last . Returns: The modified post input.","title":"InputModifier.post"},{"location":"vis.input_modifiers/#inputmodifierpre","text":"pre(self, inp) Implement pre gradient descent update modification to the input. If pre-processing is not desired, simply ignore the implementation. It returns the unmodified inp by default. Args: inp : An N-dim numpy array of shape: (samples, channels, image_dims...) if image_data_format= channels_first or (samples, image_dims..., channels) if image_data_format=channels_last . Returns: The modified pre input.","title":"InputModifier.pre"},{"location":"vis.input_modifiers/#jitter","text":"","title":"Jitter"},{"location":"vis.input_modifiers/#jitter__init__","text":"__init__(self, jitter=0.05) Implements an input modifier that introduces random jitter in pre . Jitter has been shown to produce crisper activation maximization images. Args: jitter : The amount of jitter to apply, scalar or sequence. If a scalar, same jitter is applied to all image dims. If sequence, jitter should contain a value per image dim. A value between [0., 1.] is interpreted as a percentage of the image dimension. (Default value: 0.05)","title":"Jitter.__init__"},{"location":"vis.input_modifiers/#jitterpost","text":"post(self, inp) Implement post gradient descent update modification to the input. If post-processing is not desired, simply ignore the implementation. It returns the unmodified inp by default. Args: inp : An N-dim numpy array of shape: (samples, channels, image_dims...) if image_data_format= channels_first or (samples, image_dims..., channels) if image_data_format=channels_last . Returns: The modified post input.","title":"Jitter.post"},{"location":"vis.input_modifiers/#jitterpre","text":"pre(self, img)","title":"Jitter.pre"},{"location":"vis.losses/","text":"Source: vis/losses.py#L0 Loss Abstract class for defining the loss function to be minimized. The loss function should be built by defining build_loss function. The attribute name should be defined to identify loss function with verbose outputs. Defaults to 'Unnamed Loss' if not overridden. Loss. __init__ __init__(self) Loss.build_loss build_loss(self) Implement this function to build the loss function expression. Any additional arguments required to build this loss function may be passed in via __init__ . Ideally, the function expression must be compatible with all keras backends and channels_first or channels_last image_data_format(s). utils.slicer can be used to define data format agnostic slices. (just define it in channels_first format, it will automatically shuffle indices for tensorflow which uses channels_last format). # theano slice conv_layer[:, filter_idx, ...] # TF slice conv_layer[..., filter_idx] # Backend agnostic slice conv_layer[utils.slicer[:, filter_idx, ...]] utils.get_img_shape is another optional utility that make this easier. Returns: The loss expression. ActivationMaximization A loss function that maximizes the activation of a set of filters within a particular layer. Typically this loss is used to ask the reverse question - What kind of input image would increase the networks confidence, for say, dog class. This helps determine what the network might be internalizing as being the 'dog' image space. One might also use this to generate an input image that maximizes both 'dog' and 'human' outputs on the final keras.layers.Dense layer. ActivationMaximization. __init__ __init__(self, layer, filter_indices) Args: layer : The keras layer whose filters need to be maximized. This can either be a convolutional layer or a dense layer. filter_indices : filter indices within the layer to be maximized. For keras.layers.Dense layer, filter_idx is interpreted as the output index. If you are optimizing final keras.layers.Dense layer to maximize class output, you tend to get better results with 'linear' activation as opposed to 'softmax'. This is because 'softmax' output can be maximized by minimizing scores for other classes. ActivationMaximization.build_loss build_loss(self)","title":"losses"},{"location":"vis.losses/#loss","text":"Abstract class for defining the loss function to be minimized. The loss function should be built by defining build_loss function. The attribute name should be defined to identify loss function with verbose outputs. Defaults to 'Unnamed Loss' if not overridden.","title":"Loss"},{"location":"vis.losses/#loss__init__","text":"__init__(self)","title":"Loss.__init__"},{"location":"vis.losses/#lossbuild_loss","text":"build_loss(self) Implement this function to build the loss function expression. Any additional arguments required to build this loss function may be passed in via __init__ . Ideally, the function expression must be compatible with all keras backends and channels_first or channels_last image_data_format(s). utils.slicer can be used to define data format agnostic slices. (just define it in channels_first format, it will automatically shuffle indices for tensorflow which uses channels_last format). # theano slice conv_layer[:, filter_idx, ...] # TF slice conv_layer[..., filter_idx] # Backend agnostic slice conv_layer[utils.slicer[:, filter_idx, ...]] utils.get_img_shape is another optional utility that make this easier. Returns: The loss expression.","title":"Loss.build_loss"},{"location":"vis.losses/#activationmaximization","text":"A loss function that maximizes the activation of a set of filters within a particular layer. Typically this loss is used to ask the reverse question - What kind of input image would increase the networks confidence, for say, dog class. This helps determine what the network might be internalizing as being the 'dog' image space. One might also use this to generate an input image that maximizes both 'dog' and 'human' outputs on the final keras.layers.Dense layer.","title":"ActivationMaximization"},{"location":"vis.losses/#activationmaximization__init__","text":"__init__(self, layer, filter_indices) Args: layer : The keras layer whose filters need to be maximized. This can either be a convolutional layer or a dense layer. filter_indices : filter indices within the layer to be maximized. For keras.layers.Dense layer, filter_idx is interpreted as the output index. If you are optimizing final keras.layers.Dense layer to maximize class output, you tend to get better results with 'linear' activation as opposed to 'softmax'. This is because 'softmax' output can be maximized by minimizing scores for other classes.","title":"ActivationMaximization.__init__"},{"location":"vis.losses/#activationmaximizationbuild_loss","text":"build_loss(self)","title":"ActivationMaximization.build_loss"},{"location":"vis.optimizer/","text":"Source: vis/optimizer.py#L0 Optimizer Optimizer. __init__ __init__(self, input_tensor, losses, input_range=(0, 255), wrt_tensor=None, norm_grads=True) Creates an optimizer that minimizes weighted loss function. Args: input_tensor : An input tensor of shape: (samples, channels, image_dims...) if image_data_format= channels_first or (samples, image_dims..., channels) if image_data_format=channels_last . losses : List of ( Loss , weight) tuples. input_range : Specifies the input range as a (min, max) tuple. This is used to rescale the final optimized input to the given range. (Default value=(0, 255)) wrt_tensor : Short for, with respect to. This instructs the optimizer that the aggregate loss from losses should be minimized with respect to wrt_tensor . wrt_tensor can be any tensor that is part of the model graph. Default value is set to None which means that loss will simply be minimized with respect to input_tensor . norm_grads : True to normalize gradients. Normalization avoids very small or large gradients and ensures a smooth gradient gradient descent process. If you want the actual gradient (for example, visualizing attention), set this to false. Optimizer.minimize minimize(self, seed_input=None, max_iter=200, input_modifiers=None, grad_modifier=None, \\ callbacks=None, verbose=True) Performs gradient descent on the input image with respect to defined losses. Args: seed_input : An N-dim numpy array of shape: (samples, channels, image_dims...) if image_data_format= channels_first or (samples, image_dims..., channels) if image_data_format=channels_last . Seeded with random noise if set to None. (Default value = None) max_iter : The maximum number of gradient descent iterations. (Default value = 200) input_modifiers : A list of InputModifier instances specifying how to make pre and post changes to the optimized input during the optimization process. pre is applied in list order while post is applied in reverse order. For example, input_modifiers = [f, g] means that pre_input = g(f(inp)) and post_input = f(g(inp)) grad_modifier : gradient modifier to use. See grad_modifiers . If you don't specify anything, gradients are unchanged. (Default value = None) callbacks : A list of OptimizerCallback instances to trigger. verbose : Logs individual losses at the end of every gradient descent iteration. Very useful to estimate loss weight factor(s). (Default value = True) Returns: The tuple of (optimized input, grads with respect to wrt, wrt_value) after gradient descent iterations.","title":"optimizer"},{"location":"vis.optimizer/#optimizer","text":"","title":"Optimizer"},{"location":"vis.optimizer/#optimizer__init__","text":"__init__(self, input_tensor, losses, input_range=(0, 255), wrt_tensor=None, norm_grads=True) Creates an optimizer that minimizes weighted loss function. Args: input_tensor : An input tensor of shape: (samples, channels, image_dims...) if image_data_format= channels_first or (samples, image_dims..., channels) if image_data_format=channels_last . losses : List of ( Loss , weight) tuples. input_range : Specifies the input range as a (min, max) tuple. This is used to rescale the final optimized input to the given range. (Default value=(0, 255)) wrt_tensor : Short for, with respect to. This instructs the optimizer that the aggregate loss from losses should be minimized with respect to wrt_tensor . wrt_tensor can be any tensor that is part of the model graph. Default value is set to None which means that loss will simply be minimized with respect to input_tensor . norm_grads : True to normalize gradients. Normalization avoids very small or large gradients and ensures a smooth gradient gradient descent process. If you want the actual gradient (for example, visualizing attention), set this to false.","title":"Optimizer.__init__"},{"location":"vis.optimizer/#optimizerminimize","text":"minimize(self, seed_input=None, max_iter=200, input_modifiers=None, grad_modifier=None, \\ callbacks=None, verbose=True) Performs gradient descent on the input image with respect to defined losses. Args: seed_input : An N-dim numpy array of shape: (samples, channels, image_dims...) if image_data_format= channels_first or (samples, image_dims..., channels) if image_data_format=channels_last . Seeded with random noise if set to None. (Default value = None) max_iter : The maximum number of gradient descent iterations. (Default value = 200) input_modifiers : A list of InputModifier instances specifying how to make pre and post changes to the optimized input during the optimization process. pre is applied in list order while post is applied in reverse order. For example, input_modifiers = [f, g] means that pre_input = g(f(inp)) and post_input = f(g(inp)) grad_modifier : gradient modifier to use. See grad_modifiers . If you don't specify anything, gradients are unchanged. (Default value = None) callbacks : A list of OptimizerCallback instances to trigger. verbose : Logs individual losses at the end of every gradient descent iteration. Very useful to estimate loss weight factor(s). (Default value = True) Returns: The tuple of (optimized input, grads with respect to wrt, wrt_value) after gradient descent iterations.","title":"Optimizer.minimize"},{"location":"vis.regularizers/","text":"Source: vis/regularizers.py#L0 normalize normalize(input_tensor, output_tensor) Normalizes the output_tensor with respect to input_tensor dimensions. This makes regularizer weight factor more or less uniform across various input image dimensions. Args: input_tensor : An tensor of shape: (samples, channels, image_dims...) if image_data_format= channels_first or (samples, image_dims..., channels) if image_data_format=channels_last . output_tensor : The tensor to normalize. Returns: The normalized tensor. TotalVariation TotalVariation. __init__ __init__(self, img_input, beta=2.0) Total variation regularizer encourages blobbier and coherent image structures, akin to natural images. See section 3.2.2 in Visualizing deep convolutional neural networks using natural pre-images for details. Args: img_input : An image tensor of shape: (samples, channels, image_dims...) if image_data_format= channels_first or (samples, image_dims..., channels) if image_data_format=channels_last`. beta : Smaller values of beta give sharper but 'spikier' images. Values \\in [1.5, 3.0] are recommended as a reasonable compromise. (Default value = 2.) TotalVariation.build_loss build_loss(self) Implements the N-dim version of function TV^{\\beta}(x) = \\sum_{whc} \\left ( \\left ( x(h, w+1, c) - x(h, w, c) \\right )^{2} + \\left ( x(h+1, w, c) - x(h, w, c) \\right )^{2} \\right )^{\\frac{\\beta}{2}} to return total variation for all images in the batch. LPNorm LPNorm. __init__ __init__(self, img_input, p=6.0) Builds a L-p norm function. This regularizer encourages the intensity of pixels to stay bounded. i.e., prevents pixels from taking on very large values. Args: img_input : 4D image input tensor to the model of shape: (samples, channels, rows, cols) if data_format='channels_first' or (samples, rows, cols, channels) if data_format='channels_last'. p : The pth norm to use. If p = float('inf'), infinity-norm will be used. LPNorm.build_loss build_loss(self)","title":"regularizers"},{"location":"vis.regularizers/#normalize","text":"normalize(input_tensor, output_tensor) Normalizes the output_tensor with respect to input_tensor dimensions. This makes regularizer weight factor more or less uniform across various input image dimensions. Args: input_tensor : An tensor of shape: (samples, channels, image_dims...) if image_data_format= channels_first or (samples, image_dims..., channels) if image_data_format=channels_last . output_tensor : The tensor to normalize. Returns: The normalized tensor.","title":"normalize"},{"location":"vis.regularizers/#totalvariation","text":"","title":"TotalVariation"},{"location":"vis.regularizers/#totalvariation__init__","text":"__init__(self, img_input, beta=2.0) Total variation regularizer encourages blobbier and coherent image structures, akin to natural images. See section 3.2.2 in Visualizing deep convolutional neural networks using natural pre-images for details. Args: img_input : An image tensor of shape: (samples, channels, image_dims...) if image_data_format= channels_first or (samples, image_dims..., channels) if image_data_format=channels_last`. beta : Smaller values of beta give sharper but 'spikier' images. Values \\in [1.5, 3.0] are recommended as a reasonable compromise. (Default value = 2.)","title":"TotalVariation.__init__"},{"location":"vis.regularizers/#totalvariationbuild_loss","text":"build_loss(self) Implements the N-dim version of function TV^{\\beta}(x) = \\sum_{whc} \\left ( \\left ( x(h, w+1, c) - x(h, w, c) \\right )^{2} + \\left ( x(h+1, w, c) - x(h, w, c) \\right )^{2} \\right )^{\\frac{\\beta}{2}} to return total variation for all images in the batch.","title":"TotalVariation.build_loss"},{"location":"vis.regularizers/#lpnorm","text":"","title":"LPNorm"},{"location":"vis.regularizers/#lpnorm__init__","text":"__init__(self, img_input, p=6.0) Builds a L-p norm function. This regularizer encourages the intensity of pixels to stay bounded. i.e., prevents pixels from taking on very large values. Args: img_input : 4D image input tensor to the model of shape: (samples, channels, rows, cols) if data_format='channels_first' or (samples, rows, cols, channels) if data_format='channels_last'. p : The pth norm to use. If p = float('inf'), infinity-norm will be used.","title":"LPNorm.__init__"},{"location":"vis.regularizers/#lpnormbuild_loss","text":"build_loss(self)","title":"LPNorm.build_loss"},{"location":"vis.utils.utils/","text":"Source: vis/utils/utils.py#L0 Global Variables slicer reverse_enumerate reverse_enumerate(iterable) Enumerate over an iterable in reverse order while retaining proper indexes, without creating any copies. listify listify(value) Ensures that the value is a list. If it is not a list, it creates a new list with value as an item. add_defaults_to_kwargs add_defaults_to_kwargs(defaults, **kwargs) Updates kwargs with dict of defaults Args: defaults : A dictionary of keys and values **kwargs: The kwargs to update. Returns: The updated kwargs. get_identifier get_identifier(identifier, module_globals, module_name) Helper utility to retrieve the callable function associated with a string identifier. Args: identifier : The identifier. Could be a string or function. module_globals : The global objects of the module. module_name : The module name Returns: The callable associated with the identifier. apply_modifications apply_modifications(model, custom_objects=None) Applies modifications to the model layers to create a new Graph. For example, simply changing model.layers[idx].activation = new activation does not change the graph. The entire graph needs to be updated with modified inbound and outbound tensors because of change in layer building function. Args: model : The keras.models.Model instance. Returns: The modified model with changes applied. Does not mutate the original model . random_array random_array(shape, mean=128.0, std=20.0) Creates a uniformly distributed random array with the given mean and std . Args: shape : The desired shape mean : The desired mean (Default value = 128) std : The desired std (Default value = 20) Returns: Random numpy array of given shape uniformly distributed with desired mean and std . find_layer_idx find_layer_idx(model, layer_name) Looks up the layer index corresponding to layer_name from model . Args: model : The keras.models.Model instance. layer_name : The name of the layer to lookup. Returns: The layer index if found. Raises an exception otherwise. deprocess_input deprocess_input(input_array, input_range=(0, 255)) Utility function to scale the input_array to input_range throwing away high frequency artifacts. Args: input_array : An N-dim numpy array. input_range : Specifies the input range as a (min, max) tuple to rescale the input_array . Returns: The rescaled input_array . stitch_images stitch_images(images, margin=5, cols=5) Utility function to stitch images together with a margin . Args: images : The array of 2D images to stitch. margin : The black border margin size between images (Default value = 5) cols : Max number of image cols. New row is created when number of images exceed the column size. (Default value = 5) Returns: A single numpy image array comprising of input images. get_img_shape get_img_shape(img) Returns image shape in a backend agnostic manner. Args: img : An image tensor of shape: (channels, image_dims...) if data_format='channels_first' or (image_dims..., channels) if data_format='channels_last'. Returns: Tuple containing image shape information in (samples, channels, image_dims...) order. load_img load_img(path, grayscale=False, target_size=None) Utility function to load an image from disk. Args: path : The image file path. grayscale : True to convert to grayscale image (Default value = False) target_size : (w, h) to resize. (Default value = None) Returns: The loaded numpy image. lookup_imagenet_labels lookup_imagenet_labels(indices) Utility function to return the image net label for the final dense layer output index. Args: indices : Could be a single value or an array of indices whose labels should be looked up. Returns: Image net label corresponding to the image category. draw_text draw_text(img, text, position=(10, 10), font=\"FreeSans.ttf\", font_size=14, color=(0, 0, 0)) Draws text over the image. Requires PIL. Args: img : The image to use. text : The text string to overlay. position : The text (x, y) position. (Default value = (10, 10)) font : The ttf or open type font to use. (Default value = 'FreeSans.ttf') font_size : The text font size. (Default value = 12) color : The (r, g, b) values for text color. (Default value = (0, 0, 0)) Returns: Image overlayed with text. bgr2rgb bgr2rgb(img) Converts an RGB image to BGR and vice versa Args: img : Numpy array in RGB or BGR format Returns: The converted image format normalize normalize(array, min_value=0.0, max_value=1.0) Normalizes the numpy array to (min_value, max_value) Args: array : The numpy array min_value : The min value in normalized array (Default value = 0) max_value : The max value in normalized array (Default value = 1) Returns: The array normalized to range between (min_value, max_value)","title":"utils"},{"location":"vis.utils.utils/#global-variables","text":"slicer","title":"Global Variables"},{"location":"vis.utils.utils/#reverse_enumerate","text":"reverse_enumerate(iterable) Enumerate over an iterable in reverse order while retaining proper indexes, without creating any copies.","title":"reverse_enumerate"},{"location":"vis.utils.utils/#listify","text":"listify(value) Ensures that the value is a list. If it is not a list, it creates a new list with value as an item.","title":"listify"},{"location":"vis.utils.utils/#add_defaults_to_kwargs","text":"add_defaults_to_kwargs(defaults, **kwargs) Updates kwargs with dict of defaults Args: defaults : A dictionary of keys and values **kwargs: The kwargs to update. Returns: The updated kwargs.","title":"add_defaults_to_kwargs"},{"location":"vis.utils.utils/#get_identifier","text":"get_identifier(identifier, module_globals, module_name) Helper utility to retrieve the callable function associated with a string identifier. Args: identifier : The identifier. Could be a string or function. module_globals : The global objects of the module. module_name : The module name Returns: The callable associated with the identifier.","title":"get_identifier"},{"location":"vis.utils.utils/#apply_modifications","text":"apply_modifications(model, custom_objects=None) Applies modifications to the model layers to create a new Graph. For example, simply changing model.layers[idx].activation = new activation does not change the graph. The entire graph needs to be updated with modified inbound and outbound tensors because of change in layer building function. Args: model : The keras.models.Model instance. Returns: The modified model with changes applied. Does not mutate the original model .","title":"apply_modifications"},{"location":"vis.utils.utils/#random_array","text":"random_array(shape, mean=128.0, std=20.0) Creates a uniformly distributed random array with the given mean and std . Args: shape : The desired shape mean : The desired mean (Default value = 128) std : The desired std (Default value = 20) Returns: Random numpy array of given shape uniformly distributed with desired mean and std .","title":"random_array"},{"location":"vis.utils.utils/#find_layer_idx","text":"find_layer_idx(model, layer_name) Looks up the layer index corresponding to layer_name from model . Args: model : The keras.models.Model instance. layer_name : The name of the layer to lookup. Returns: The layer index if found. Raises an exception otherwise.","title":"find_layer_idx"},{"location":"vis.utils.utils/#deprocess_input","text":"deprocess_input(input_array, input_range=(0, 255)) Utility function to scale the input_array to input_range throwing away high frequency artifacts. Args: input_array : An N-dim numpy array. input_range : Specifies the input range as a (min, max) tuple to rescale the input_array . Returns: The rescaled input_array .","title":"deprocess_input"},{"location":"vis.utils.utils/#stitch_images","text":"stitch_images(images, margin=5, cols=5) Utility function to stitch images together with a margin . Args: images : The array of 2D images to stitch. margin : The black border margin size between images (Default value = 5) cols : Max number of image cols. New row is created when number of images exceed the column size. (Default value = 5) Returns: A single numpy image array comprising of input images.","title":"stitch_images"},{"location":"vis.utils.utils/#get_img_shape","text":"get_img_shape(img) Returns image shape in a backend agnostic manner. Args: img : An image tensor of shape: (channels, image_dims...) if data_format='channels_first' or (image_dims..., channels) if data_format='channels_last'. Returns: Tuple containing image shape information in (samples, channels, image_dims...) order.","title":"get_img_shape"},{"location":"vis.utils.utils/#load_img","text":"load_img(path, grayscale=False, target_size=None) Utility function to load an image from disk. Args: path : The image file path. grayscale : True to convert to grayscale image (Default value = False) target_size : (w, h) to resize. (Default value = None) Returns: The loaded numpy image.","title":"load_img"},{"location":"vis.utils.utils/#lookup_imagenet_labels","text":"lookup_imagenet_labels(indices) Utility function to return the image net label for the final dense layer output index. Args: indices : Could be a single value or an array of indices whose labels should be looked up. Returns: Image net label corresponding to the image category.","title":"lookup_imagenet_labels"},{"location":"vis.utils.utils/#draw_text","text":"draw_text(img, text, position=(10, 10), font=\"FreeSans.ttf\", font_size=14, color=(0, 0, 0)) Draws text over the image. Requires PIL. Args: img : The image to use. text : The text string to overlay. position : The text (x, y) position. (Default value = (10, 10)) font : The ttf or open type font to use. (Default value = 'FreeSans.ttf') font_size : The text font size. (Default value = 12) color : The (r, g, b) values for text color. (Default value = (0, 0, 0)) Returns: Image overlayed with text.","title":"draw_text"},{"location":"vis.utils.utils/#bgr2rgb","text":"bgr2rgb(img) Converts an RGB image to BGR and vice versa Args: img : Numpy array in RGB or BGR format Returns: The converted image format","title":"bgr2rgb"},{"location":"vis.utils.utils/#normalize","text":"normalize(array, min_value=0.0, max_value=1.0) Normalizes the numpy array to (min_value, max_value) Args: array : The numpy array min_value : The min value in normalized array (Default value = 0) max_value : The max value in normalized array (Default value = 1) Returns: The array normalized to range between (min_value, max_value)","title":"normalize"},{"location":"vis.visualization/","text":"Source: vis/visualization/ init .py#L0 visualize_activation_with_losses visualize_activation_with_losses(input_tensor, losses, wrt_tensor=None, seed_input=None, \\ input_range=(0, 255), **optimizer_params) Generates the input_tensor that minimizes the weighted losses . This function is intended for advanced use cases where a custom loss is desired. Args: input_tensor : An input tensor of shape: (samples, channels, image_dims...) if image_data_format= channels_first or (samples, image_dims..., channels) if image_data_format=channels_last . wrt_tensor : Short for, with respect to. The gradients of losses are computed with respect to this tensor. When None, this is assumed to be the same as input_tensor (Default value: None) losses : List of ( Loss , weight) tuples. seed_input : Seeds the optimization with a starting image. Initialized with a random value when set to None. (Default value = None) input_range : Specifies the input range as a (min, max) tuple. This is used to rescale the final optimized input to the given range. (Default value=(0, 255)) optimizer_params : The **kwargs for optimizer params . Will default to reasonable values when required keys are not found. Returns: The model input that minimizes the weighted losses . get_num_filters get_num_filters(layer) Determines the number of filters within the given layer . Args: layer : The keras layer to use. Returns: Total number of filters within layer . For keras.layers.Dense layer, this is the total number of outputs. overlay overlay(array1, array2, alpha=0.5) Overlays array1 onto array2 with alpha blending. Args: array1 : The first numpy array. array2 : The second numpy array. alpha : The alpha value of array1 as overlayed onto array2 . This value needs to be between [0, 1], with 0 being array2 only to 1 being array1 only (Default value = 0.5). Returns: The array1 , overlayed with array2 using alpha blending. visualize_saliency_with_losses visualize_saliency_with_losses(input_tensor, losses, seed_input, wrt_tensor=None, \\ grad_modifier=\"absolute\", keepdims=False) Generates an attention heatmap over the seed_input by using positive gradients of input_tensor with respect to weighted losses . This function is intended for advanced use cases where a custom loss is desired. For common use cases, refer to visualize_class_saliency or visualize_regression_saliency . For a full description of saliency, see the paper: [Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps] (https://arxiv.org/pdf/1312.6034v2.pdf) Args: input_tensor : An input tensor of shape: (samples, channels, image_dims...) if image_data_format= channels_first or (samples, image_dims..., channels) if image_data_format=channels_last . losses : List of ( Loss , weight) tuples. seed_input : The model input for which activation map needs to be visualized. wrt_tensor : Short for, with respect to. The gradients of losses are computed with respect to this tensor. When None, this is assumed to be the same as input_tensor (Default value: None) grad_modifier : gradient modifier to use. See grad_modifiers . By default absolute value of gradients are used. To visualize positive or negative gradients, use relu and negate respectively. (Default value = 'absolute') keepdims : A boolean, whether to keep the dimensions or not. If keepdims is False, the channels axis is deleted. If keepdims is True, the grad with same shape as input_tensor is returned. (Default value: False) Returns: The normalized gradients of seed_input with respect to weighted losses . visualize_activation visualize_activation(model, layer_idx, filter_indices=None, wrt_tensor=None, seed_input=None, \\ input_range=(0, 255), backprop_modifier=None, grad_modifier=None, act_max_weight=1, \\ lp_norm_weight=10, tv_weight=10, **optimizer_params) Generates the model input that maximizes the output of all filter_indices in the given layer_idx . Args: model : The keras.models.Model instance. The model input shape must be: (samples, channels, image_dims...) if image_data_format=channels_first or (samples, image_dims..., channels) if image_data_format=channels_last . layer_idx : The layer index within model.layers whose filters needs to be visualized. filter_indices : filter indices within the layer to be maximized. If None, all filters are visualized. (Default value = None) For keras.layers.Dense layer, filter_idx is interpreted as the output index. If you are visualizing final keras.layers.Dense layer, consider switching 'softmax' activation for 'linear' using utils.apply_modifications for better results. wrt_tensor : Short for, with respect to. The gradients of losses are computed with respect to this tensor. When None, this is assumed to be the same as input_tensor (Default value: None) seed_input : Seeds the optimization with a starting input. Initialized with a random value when set to None. (Default value = None) input_range : Specifies the input range as a (min, max) tuple. This is used to rescale the final optimized input to the given range. (Default value=(0, 255)) backprop_modifier : backprop modifier to use. See backprop_modifiers . If you don't specify anything, no backprop modification is applied. (Default value = None) grad_modifier : gradient modifier to use. See grad_modifiers . If you don't specify anything, gradients are unchanged (Default value = None) act_max_weight : The weight param for ActivationMaximization loss. Not used if 0 or None. (Default value = 1) lp_norm_weight : The weight param for LPNorm regularization loss. Not used if 0 or None. (Default value = 10) tv_weight : The weight param for TotalVariation regularization loss. Not used if 0 or None. (Default value = 10) optimizer_params : The **kwargs for optimizer params . Will default to reasonable values when required keys are not found. Example: If you wanted to visualize the input image that would maximize the output index 22, say on final keras.layers.Dense layer, then, filter_indices = [22] , layer_idx = dense_layer_idx . If filter_indices = [22, 23] , then it should generate an input image that shows features of both classes. Returns: The model input that maximizes the output of filter_indices in the given layer_idx . visualize_saliency visualize_saliency(model, layer_idx, filter_indices, seed_input, wrt_tensor=None, \\ backprop_modifier=None, grad_modifier=\"absolute\", keepdims=False) Generates an attention heatmap over the seed_input for maximizing filter_indices output in the given layer_idx . Args: model : The keras.models.Model instance. The model input shape must be: (samples, channels, image_dims...) if image_data_format=channels_first or (samples, image_dims..., channels) if image_data_format=channels_last . layer_idx : The layer index within model.layers whose filters needs to be visualized. filter_indices : filter indices within the layer to be maximized. If None, all filters are visualized. (Default value = None) For keras.layers.Dense layer, filter_idx is interpreted as the output index. If you are visualizing final keras.layers.Dense layer, consider switching 'softmax' activation for 'linear' using utils.apply_modifications for better results. seed_input : The model input for which activation map needs to be visualized. wrt_tensor : Short for, with respect to. The gradients of losses are computed with respect to this tensor. When None, this is assumed to be the same as input_tensor (Default value: None) backprop_modifier : backprop modifier to use. See backprop_modifiers . If you don't specify anything, no backprop modification is applied. (Default value = None) grad_modifier : gradient modifier to use. See grad_modifiers . By default absolute value of gradients are used. To visualize positive or negative gradients, use relu and negate respectively. (Default value = 'absolute') keepdims : A boolean, whether to keep the dimensions or not. If keepdims is False, the channels axis is deleted. If keepdims is True, the grad with same shape as input_tensor is returned. (Default value: False) Example: If you wanted to visualize attention over 'bird' category, say output index 22 on the final keras.layers.Dense layer, then, filter_indices = [22] , layer = dense_layer . One could also set filter indices to more than one value. For example, filter_indices = [22, 23] should (hopefully) show attention map that corresponds to both 22, 23 output categories. Returns: The heatmap image indicating the seed_input regions whose change would most contribute towards maximizing the output of filter_indices . visualize_cam_with_losses visualize_cam_with_losses(input_tensor, losses, seed_input, penultimate_layer, grad_modifier=None) Generates a gradient based class activation map (CAM) by using positive gradients of input_tensor with respect to weighted losses . For details on grad-CAM, see the paper: [Grad-CAM: Why did you say that? Visual Explanations from Deep Networks via Gradient-based Localization] (https://arxiv.org/pdf/1610.02391v1.pdf). Unlike class activation mapping , which requires minor changes to network architecture in some instances, grad-CAM has a more general applicability. Compared to saliency maps, grad-CAM is class discriminative; i.e., the 'cat' explanation exclusively highlights cat regions and not the 'dog' region and vice-versa. Args: input_tensor : An input tensor of shape: (samples, channels, image_dims...) if image_data_format= channels_first or (samples, image_dims..., channels) if image_data_format=channels_last . losses : List of ( Loss , weight) tuples. seed_input : The model input for which activation map needs to be visualized. penultimate_layer : The pre-layer to layer_idx whose feature maps should be used to compute gradients with respect to filter output. grad_modifier : gradient modifier to use. See grad_modifiers . If you don't specify anything, gradients are unchanged (Default value = None) Returns: The normalized gradients of seed_input with respect to weighted losses . visualize_cam visualize_cam(model, layer_idx, filter_indices, seed_input, penultimate_layer_idx=None, \\ backprop_modifier=None, grad_modifier=None) Generates a gradient based class activation map (grad-CAM) that maximizes the outputs of filter_indices in layer_idx . Args: model : The keras.models.Model instance. The model input shape must be: (samples, channels, image_dims...) if image_data_format=channels_first or (samples, image_dims..., channels) if image_data_format=channels_last . layer_idx : The layer index within model.layers whose filters needs to be visualized. filter_indices : filter indices within the layer to be maximized. If None, all filters are visualized. (Default value = None) For keras.layers.Dense layer, filter_idx is interpreted as the output index. If you are visualizing final keras.layers.Dense layer, consider switching 'softmax' activation for 'linear' using utils.apply_modifications for better results. seed_input : The input image for which activation map needs to be visualized. penultimate_layer_idx : The pre-layer to layer_idx whose feature maps should be used to compute gradients wrt filter output. If not provided, it is set to the nearest penultimate Conv or Pooling layer. backprop_modifier : backprop modifier to use. See backprop_modifiers . If you don't specify anything, no backprop modification is applied. (Default value = None) grad_modifier : gradient modifier to use. See grad_modifiers . If you don't specify anything, gradients are unchanged (Default value = None) Example: If you wanted to visualize attention over 'bird' category, say output index 22 on the final keras.layers.Dense layer, then, filter_indices = [22] , layer = dense_layer . One could also set filter indices to more than one value. For example, filter_indices = [22, 23] should (hopefully) show attention map that corresponds to both 22, 23 output categories. Returns: The heatmap image indicating the input regions whose change would most contribute towards maximizing the output of filter_indices .","title":"Visualization"},{"location":"vis.visualization/#visualize_activation_with_losses","text":"visualize_activation_with_losses(input_tensor, losses, wrt_tensor=None, seed_input=None, \\ input_range=(0, 255), **optimizer_params) Generates the input_tensor that minimizes the weighted losses . This function is intended for advanced use cases where a custom loss is desired. Args: input_tensor : An input tensor of shape: (samples, channels, image_dims...) if image_data_format= channels_first or (samples, image_dims..., channels) if image_data_format=channels_last . wrt_tensor : Short for, with respect to. The gradients of losses are computed with respect to this tensor. When None, this is assumed to be the same as input_tensor (Default value: None) losses : List of ( Loss , weight) tuples. seed_input : Seeds the optimization with a starting image. Initialized with a random value when set to None. (Default value = None) input_range : Specifies the input range as a (min, max) tuple. This is used to rescale the final optimized input to the given range. (Default value=(0, 255)) optimizer_params : The **kwargs for optimizer params . Will default to reasonable values when required keys are not found. Returns: The model input that minimizes the weighted losses .","title":"visualize_activation_with_losses"},{"location":"vis.visualization/#get_num_filters","text":"get_num_filters(layer) Determines the number of filters within the given layer . Args: layer : The keras layer to use. Returns: Total number of filters within layer . For keras.layers.Dense layer, this is the total number of outputs.","title":"get_num_filters"},{"location":"vis.visualization/#overlay","text":"overlay(array1, array2, alpha=0.5) Overlays array1 onto array2 with alpha blending. Args: array1 : The first numpy array. array2 : The second numpy array. alpha : The alpha value of array1 as overlayed onto array2 . This value needs to be between [0, 1], with 0 being array2 only to 1 being array1 only (Default value = 0.5). Returns: The array1 , overlayed with array2 using alpha blending.","title":"overlay"},{"location":"vis.visualization/#visualize_saliency_with_losses","text":"visualize_saliency_with_losses(input_tensor, losses, seed_input, wrt_tensor=None, \\ grad_modifier=\"absolute\", keepdims=False) Generates an attention heatmap over the seed_input by using positive gradients of input_tensor with respect to weighted losses . This function is intended for advanced use cases where a custom loss is desired. For common use cases, refer to visualize_class_saliency or visualize_regression_saliency . For a full description of saliency, see the paper: [Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps] (https://arxiv.org/pdf/1312.6034v2.pdf) Args: input_tensor : An input tensor of shape: (samples, channels, image_dims...) if image_data_format= channels_first or (samples, image_dims..., channels) if image_data_format=channels_last . losses : List of ( Loss , weight) tuples. seed_input : The model input for which activation map needs to be visualized. wrt_tensor : Short for, with respect to. The gradients of losses are computed with respect to this tensor. When None, this is assumed to be the same as input_tensor (Default value: None) grad_modifier : gradient modifier to use. See grad_modifiers . By default absolute value of gradients are used. To visualize positive or negative gradients, use relu and negate respectively. (Default value = 'absolute') keepdims : A boolean, whether to keep the dimensions or not. If keepdims is False, the channels axis is deleted. If keepdims is True, the grad with same shape as input_tensor is returned. (Default value: False) Returns: The normalized gradients of seed_input with respect to weighted losses .","title":"visualize_saliency_with_losses"},{"location":"vis.visualization/#visualize_activation","text":"visualize_activation(model, layer_idx, filter_indices=None, wrt_tensor=None, seed_input=None, \\ input_range=(0, 255), backprop_modifier=None, grad_modifier=None, act_max_weight=1, \\ lp_norm_weight=10, tv_weight=10, **optimizer_params) Generates the model input that maximizes the output of all filter_indices in the given layer_idx . Args: model : The keras.models.Model instance. The model input shape must be: (samples, channels, image_dims...) if image_data_format=channels_first or (samples, image_dims..., channels) if image_data_format=channels_last . layer_idx : The layer index within model.layers whose filters needs to be visualized. filter_indices : filter indices within the layer to be maximized. If None, all filters are visualized. (Default value = None) For keras.layers.Dense layer, filter_idx is interpreted as the output index. If you are visualizing final keras.layers.Dense layer, consider switching 'softmax' activation for 'linear' using utils.apply_modifications for better results. wrt_tensor : Short for, with respect to. The gradients of losses are computed with respect to this tensor. When None, this is assumed to be the same as input_tensor (Default value: None) seed_input : Seeds the optimization with a starting input. Initialized with a random value when set to None. (Default value = None) input_range : Specifies the input range as a (min, max) tuple. This is used to rescale the final optimized input to the given range. (Default value=(0, 255)) backprop_modifier : backprop modifier to use. See backprop_modifiers . If you don't specify anything, no backprop modification is applied. (Default value = None) grad_modifier : gradient modifier to use. See grad_modifiers . If you don't specify anything, gradients are unchanged (Default value = None) act_max_weight : The weight param for ActivationMaximization loss. Not used if 0 or None. (Default value = 1) lp_norm_weight : The weight param for LPNorm regularization loss. Not used if 0 or None. (Default value = 10) tv_weight : The weight param for TotalVariation regularization loss. Not used if 0 or None. (Default value = 10) optimizer_params : The **kwargs for optimizer params . Will default to reasonable values when required keys are not found. Example: If you wanted to visualize the input image that would maximize the output index 22, say on final keras.layers.Dense layer, then, filter_indices = [22] , layer_idx = dense_layer_idx . If filter_indices = [22, 23] , then it should generate an input image that shows features of both classes. Returns: The model input that maximizes the output of filter_indices in the given layer_idx .","title":"visualize_activation"},{"location":"vis.visualization/#visualize_saliency","text":"visualize_saliency(model, layer_idx, filter_indices, seed_input, wrt_tensor=None, \\ backprop_modifier=None, grad_modifier=\"absolute\", keepdims=False) Generates an attention heatmap over the seed_input for maximizing filter_indices output in the given layer_idx . Args: model : The keras.models.Model instance. The model input shape must be: (samples, channels, image_dims...) if image_data_format=channels_first or (samples, image_dims..., channels) if image_data_format=channels_last . layer_idx : The layer index within model.layers whose filters needs to be visualized. filter_indices : filter indices within the layer to be maximized. If None, all filters are visualized. (Default value = None) For keras.layers.Dense layer, filter_idx is interpreted as the output index. If you are visualizing final keras.layers.Dense layer, consider switching 'softmax' activation for 'linear' using utils.apply_modifications for better results. seed_input : The model input for which activation map needs to be visualized. wrt_tensor : Short for, with respect to. The gradients of losses are computed with respect to this tensor. When None, this is assumed to be the same as input_tensor (Default value: None) backprop_modifier : backprop modifier to use. See backprop_modifiers . If you don't specify anything, no backprop modification is applied. (Default value = None) grad_modifier : gradient modifier to use. See grad_modifiers . By default absolute value of gradients are used. To visualize positive or negative gradients, use relu and negate respectively. (Default value = 'absolute') keepdims : A boolean, whether to keep the dimensions or not. If keepdims is False, the channels axis is deleted. If keepdims is True, the grad with same shape as input_tensor is returned. (Default value: False) Example: If you wanted to visualize attention over 'bird' category, say output index 22 on the final keras.layers.Dense layer, then, filter_indices = [22] , layer = dense_layer . One could also set filter indices to more than one value. For example, filter_indices = [22, 23] should (hopefully) show attention map that corresponds to both 22, 23 output categories. Returns: The heatmap image indicating the seed_input regions whose change would most contribute towards maximizing the output of filter_indices .","title":"visualize_saliency"},{"location":"vis.visualization/#visualize_cam_with_losses","text":"visualize_cam_with_losses(input_tensor, losses, seed_input, penultimate_layer, grad_modifier=None) Generates a gradient based class activation map (CAM) by using positive gradients of input_tensor with respect to weighted losses . For details on grad-CAM, see the paper: [Grad-CAM: Why did you say that? Visual Explanations from Deep Networks via Gradient-based Localization] (https://arxiv.org/pdf/1610.02391v1.pdf). Unlike class activation mapping , which requires minor changes to network architecture in some instances, grad-CAM has a more general applicability. Compared to saliency maps, grad-CAM is class discriminative; i.e., the 'cat' explanation exclusively highlights cat regions and not the 'dog' region and vice-versa. Args: input_tensor : An input tensor of shape: (samples, channels, image_dims...) if image_data_format= channels_first or (samples, image_dims..., channels) if image_data_format=channels_last . losses : List of ( Loss , weight) tuples. seed_input : The model input for which activation map needs to be visualized. penultimate_layer : The pre-layer to layer_idx whose feature maps should be used to compute gradients with respect to filter output. grad_modifier : gradient modifier to use. See grad_modifiers . If you don't specify anything, gradients are unchanged (Default value = None) Returns: The normalized gradients of seed_input with respect to weighted losses .","title":"visualize_cam_with_losses"},{"location":"vis.visualization/#visualize_cam","text":"visualize_cam(model, layer_idx, filter_indices, seed_input, penultimate_layer_idx=None, \\ backprop_modifier=None, grad_modifier=None) Generates a gradient based class activation map (grad-CAM) that maximizes the outputs of filter_indices in layer_idx . Args: model : The keras.models.Model instance. The model input shape must be: (samples, channels, image_dims...) if image_data_format=channels_first or (samples, image_dims..., channels) if image_data_format=channels_last . layer_idx : The layer index within model.layers whose filters needs to be visualized. filter_indices : filter indices within the layer to be maximized. If None, all filters are visualized. (Default value = None) For keras.layers.Dense layer, filter_idx is interpreted as the output index. If you are visualizing final keras.layers.Dense layer, consider switching 'softmax' activation for 'linear' using utils.apply_modifications for better results. seed_input : The input image for which activation map needs to be visualized. penultimate_layer_idx : The pre-layer to layer_idx whose feature maps should be used to compute gradients wrt filter output. If not provided, it is set to the nearest penultimate Conv or Pooling layer. backprop_modifier : backprop modifier to use. See backprop_modifiers . If you don't specify anything, no backprop modification is applied. (Default value = None) grad_modifier : gradient modifier to use. See grad_modifiers . If you don't specify anything, gradients are unchanged (Default value = None) Example: If you wanted to visualize attention over 'bird' category, say output index 22 on the final keras.layers.Dense layer, then, filter_indices = [22] , layer = dense_layer . One could also set filter indices to more than one value. For example, filter_indices = [22, 23] should (hopefully) show attention map that corresponds to both 22, 23 output categories. Returns: The heatmap image indicating the input regions whose change would most contribute towards maximizing the output of filter_indices .","title":"visualize_cam"},{"location":"visualizations/activation_maximization/","text":"What is Activation Maximization? In a CNN, each Conv layer has several learned template matching filters that maximize their output when a similar template pattern is found in the input image. First Conv layer is easy to interpret; simply visualize the weights as an image. To see what the Conv layer is doing, a simple option is to apply the filter over raw input pixels. Subsequent Conv filters operate over the outputs of previous Conv filters (which indicate the presence or absence of some templates), making them hard to interpret. The idea behind activation maximization is simple in hindsight - Generate an input image that maximizes the filter output activations. i.e., we compute \\frac{\\partial ActivationMaximizationLoss}{\\partial input} and use that estimate to update the input. ActivationMaximization loss simply outputs small values for large filter activations (we are minimizing losses during gradient descent iterations). This allows us to understand what sort of input patterns activate a particular filter. For example, there could be an eye filter that activates for the presence of eye within the input image. Usage There are two APIs exposed to perform activation maximization. visualize_activation : This is the general purpose API for visualizing activations. visualize_activation_with_losses : This is intended for research use-cases where some custom weighted losses can be minimized. See examples/ for code examples. Scenarios The API is very general purpose and can be used in a wide variety of scenarios. We will list the most common use-cases below: Categorical Output Dense layer visualization How can we assess whether a network is over/under fitting or generalizing well? Given an input image, a CNN can classify whether it is a cat, bird etc. How can we be sure that it is capturing the correct notion of what it means to be a bird? One way to answer these questions is to pose the reverse question: Generate an input image that maximizes the final Dense layer output corresponding to bird class. This can be done by pointing layer_idx to final Dense layer, and setting filter_indices to the desired output category. For multi-class classification, filter_indices can point to a single class. You could point also point it to multiple categories to see what a cat-fish might look like, as an example. For multi-label classifier, simply set the appropriate filter_indices . Regression Output Dense layer visualization Unlike class activation visualizations, for regression outputs, we could visualize input that increases decreases the regressed filter_indices output. For example, if you trained an apple counter model, increasing the regression output should correspond to more apples showing up in the input image. Similarly one could decrease the current output. This can be achieved by using grad_modifier option. As the name suggests, it is used to modify the gradient of losses with respect to inputs. By default, ActivationMaximization loss is used to increase the output. By setting grad_modifier='negate' you can negate the gradients, thus causing output values to decrease. gradient_modifiers are very powerful and show up in other visualization APIs as well. Conv filter visualization By pointing layer_idx to Conv layer, you can visualize what pattern activates a filter. This might help you discover what a filter might be computing. Here, filter_indices refers to the index of the Conv filter within the layer. Advanced usage backprop_modifiers allow you to modify the backpropagation behavior. For examples, you could tweak backprop to only propagate positive gradients by using backprop_modifier='relu' . This parameter also accepts a function and can be used to implement your crazy research idea :) Tips and tricks If you get garbage visualization, try setting verbose=True to see various losses during gradient descent iterations. By default, visualize_activation uses TotalVariation and LpNorm regularization to enforce natural image prior. It is very likely that you would see ActivationMaximization Loss bounce back and forth as they are dominated by regularization loss weights. Try setting all weights to zero and gradually try increasing values of total variation weight. To get sharper looking images, use Jitter input modifier. Regression models usually do not provide enough gradient information to generate meaningful input images. Try seeding the input using seed_input and see if the modifications to the input make sense. Consider submitting a PR to add more tips and tricks that you found useful.","title":"Activation Maximization"},{"location":"visualizations/activation_maximization/#what-is-activation-maximization","text":"In a CNN, each Conv layer has several learned template matching filters that maximize their output when a similar template pattern is found in the input image. First Conv layer is easy to interpret; simply visualize the weights as an image. To see what the Conv layer is doing, a simple option is to apply the filter over raw input pixels. Subsequent Conv filters operate over the outputs of previous Conv filters (which indicate the presence or absence of some templates), making them hard to interpret. The idea behind activation maximization is simple in hindsight - Generate an input image that maximizes the filter output activations. i.e., we compute \\frac{\\partial ActivationMaximizationLoss}{\\partial input} and use that estimate to update the input. ActivationMaximization loss simply outputs small values for large filter activations (we are minimizing losses during gradient descent iterations). This allows us to understand what sort of input patterns activate a particular filter. For example, there could be an eye filter that activates for the presence of eye within the input image.","title":"What is Activation Maximization?"},{"location":"visualizations/activation_maximization/#usage","text":"There are two APIs exposed to perform activation maximization. visualize_activation : This is the general purpose API for visualizing activations. visualize_activation_with_losses : This is intended for research use-cases where some custom weighted losses can be minimized. See examples/ for code examples.","title":"Usage"},{"location":"visualizations/activation_maximization/#scenarios","text":"The API is very general purpose and can be used in a wide variety of scenarios. We will list the most common use-cases below:","title":"Scenarios"},{"location":"visualizations/activation_maximization/#categorical-output-dense-layer-visualization","text":"How can we assess whether a network is over/under fitting or generalizing well? Given an input image, a CNN can classify whether it is a cat, bird etc. How can we be sure that it is capturing the correct notion of what it means to be a bird? One way to answer these questions is to pose the reverse question: Generate an input image that maximizes the final Dense layer output corresponding to bird class. This can be done by pointing layer_idx to final Dense layer, and setting filter_indices to the desired output category. For multi-class classification, filter_indices can point to a single class. You could point also point it to multiple categories to see what a cat-fish might look like, as an example. For multi-label classifier, simply set the appropriate filter_indices .","title":"Categorical Output Dense layer visualization"},{"location":"visualizations/activation_maximization/#regression-output-dense-layer-visualization","text":"Unlike class activation visualizations, for regression outputs, we could visualize input that increases decreases the regressed filter_indices output. For example, if you trained an apple counter model, increasing the regression output should correspond to more apples showing up in the input image. Similarly one could decrease the current output. This can be achieved by using grad_modifier option. As the name suggests, it is used to modify the gradient of losses with respect to inputs. By default, ActivationMaximization loss is used to increase the output. By setting grad_modifier='negate' you can negate the gradients, thus causing output values to decrease. gradient_modifiers are very powerful and show up in other visualization APIs as well.","title":"Regression Output Dense layer visualization"},{"location":"visualizations/activation_maximization/#conv-filter-visualization","text":"By pointing layer_idx to Conv layer, you can visualize what pattern activates a filter. This might help you discover what a filter might be computing. Here, filter_indices refers to the index of the Conv filter within the layer.","title":"Conv filter visualization"},{"location":"visualizations/activation_maximization/#advanced-usage","text":"backprop_modifiers allow you to modify the backpropagation behavior. For examples, you could tweak backprop to only propagate positive gradients by using backprop_modifier='relu' . This parameter also accepts a function and can be used to implement your crazy research idea :)","title":"Advanced usage"},{"location":"visualizations/activation_maximization/#tips-and-tricks","text":"If you get garbage visualization, try setting verbose=True to see various losses during gradient descent iterations. By default, visualize_activation uses TotalVariation and LpNorm regularization to enforce natural image prior. It is very likely that you would see ActivationMaximization Loss bounce back and forth as they are dominated by regularization loss weights. Try setting all weights to zero and gradually try increasing values of total variation weight. To get sharper looking images, use Jitter input modifier. Regression models usually do not provide enough gradient information to generate meaningful input images. Try seeding the input using seed_input and see if the modifications to the input make sense. Consider submitting a PR to add more tips and tricks that you found useful.","title":"Tips and tricks"},{"location":"visualizations/class_activation_maps/","text":"What is a Class Activation Map? Class activation maps or grad-CAM is another way of visualizing attention over input. Instead of using gradients with respect to output (see saliency ), grad-CAM uses penultimate (pre Dense layer) Conv layer output. The intuition is to use the nearest Conv layer to utilize spatial information that gets completely lost in Dense layers. In keras-vis, we use grad-CAM as its considered more general than Class Activation maps . Usage There are two APIs exposed to visualize grad-CAM and are almost identical to saliency usage . visualize_cam : This is the general purpose API for visualizing grad-CAM. visualize_cam_with_losses : This is intended for research use-cases where some custom weighted loss can be used. The only notable addition is the penultimate_layer_idx parameter. This can be used to specify the pre-layer whose output gradients are used. By default, keras-vis will search for the nearest layer with filters. Scenarios See saliency scenarios . Everything is identical expect the added penultimate_layer_idx param. Gotchas grad-CAM only works well if the penultimate layer is close to the layer being visualized. This also applies to Conv filter visualizations. You are better off using saliency of this is not the case with your model.","title":"Class Activation Maps"},{"location":"visualizations/class_activation_maps/#what-is-a-class-activation-map","text":"Class activation maps or grad-CAM is another way of visualizing attention over input. Instead of using gradients with respect to output (see saliency ), grad-CAM uses penultimate (pre Dense layer) Conv layer output. The intuition is to use the nearest Conv layer to utilize spatial information that gets completely lost in Dense layers. In keras-vis, we use grad-CAM as its considered more general than Class Activation maps .","title":"What is a Class Activation Map?"},{"location":"visualizations/class_activation_maps/#usage","text":"There are two APIs exposed to visualize grad-CAM and are almost identical to saliency usage . visualize_cam : This is the general purpose API for visualizing grad-CAM. visualize_cam_with_losses : This is intended for research use-cases where some custom weighted loss can be used. The only notable addition is the penultimate_layer_idx parameter. This can be used to specify the pre-layer whose output gradients are used. By default, keras-vis will search for the nearest layer with filters.","title":"Usage"},{"location":"visualizations/class_activation_maps/#scenarios","text":"See saliency scenarios . Everything is identical expect the added penultimate_layer_idx param.","title":"Scenarios"},{"location":"visualizations/class_activation_maps/#gotchas","text":"grad-CAM only works well if the penultimate layer is close to the layer being visualized. This also applies to Conv filter visualizations. You are better off using saliency of this is not the case with your model.","title":"Gotchas"},{"location":"visualizations/saliency/","text":"What is Saliency? Suppose that all the training images of bird class contains a tree with leaves. How do we know whether the CNN is using bird-related pixels, as opposed to some other features such as the tree or leaves in the image? This actually happens more often than you think and you should be especially suspicious if you have a small training set. Saliency maps was first introduced in the paper: Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps The idea is pretty simple. We compute the gradient of output category with respect to input image. This should tell us how output category value changes with respect to a small change in input image pixels. All the positive values in the gradients tell us that a small change to that pixel will increase the output value. Hence, visualizing these gradients, which are the same shape as the image should provide some intuition of attention. The idea behind saliency is pretty simple in hindsight. We compute the gradient of output category with respect to input image. \\frac{\\partial output}{\\partial input} This should tell us how the output value changes with respect to a small change in inputs. We can use these gradients to highlight input regions that cause the most change in the output. Intuitively this should highlight salient image regions that most contribute towards the output. Usage There are two APIs exposed to visualize saliency. visualize_saliency : This is the general purpose API for visualizing saliency. visualize_saliency_with_losses : This is intended for research use-cases where some custom weighted loss can be used. See examples/ for code examples. Scenarios The API is very general purpose and can be used in a wide variety of scenarios. We will list the most common use-cases below: Categorical Dense layer visualization By setting layer_idx to final Dense layer, and filter_indices to the desired output category, we can visualize parts of the seed_input that contribute most towards activating the corresponding output nodes, For multi-class classification, filter_indices can point to a single class. For multi-label classifier, simply set the appropriate filter_indices . Regression Dense layer visualization For regression outputs, we could visualize attention over input that increases decreases maintains the regressed filter_indices output. For example, consider a self driving model with continuous regression steering output. One could visualize parts of the seed_input that contributes towards increase, decrease or maintenance of predicted output. By default, saliency tells us how to increase the output activations. For the self driving car case, this only tells us parts of the input image that contribute towards steering angle increase. Other use cases can be visualized by using grad_modifier option. As the name suggests, it is used to modify the gradient of losses with respect to inputs. To visualize decrease in output, use grad_modifier='negate' . By default, ActivationMaximization loss yields positive gradients for inputs regions that increase the output. By setting grad_modifier='negate' you can treat negative gradients (which indicate the decrease) as positive and therefore visualize decrease use case. To visualize what contributed to the predicted output, we want to consider gradients that have very low positive or negative values. This can be achieved by performing grads = abs(1 / grads) to magnifies small gradients. Equivalently, you can use grad_modifier='small_values' , which does the same thing. gradient_modifiers are very powerful and show up in other visualization APIs as well. You can see a practical application for this in the self diving car example. Guided / rectified saliency Zieler et al. has the idea of clipping negative gradients in the backprop step. i.e., only propagate positive gradient information that communicates the increase in output. We call this rectified or deconv saliency. Details can be found in the paper: Visualizing and Understanding Convolutional Networks . In guided saliency, the backprop step is modified to only propagate positive gradients for positive activations. For details see the paper: String For Simplicity: The All Convolutional Net . For both these cases, we can use backprop_modifier='relu' and backprop_modifier='guided' respectively. You can also implement your own backprop_modifier to try your crazy research idea :) Conv filter saliency By pointing layer_idx to Conv layer, you can visualize parts of the image that influence the filter. This might help you discover what a filter cares about. Here, filter_indices refers to the index of the Conv filter within the layer.","title":"Saliency Maps"},{"location":"visualizations/saliency/#what-is-saliency","text":"Suppose that all the training images of bird class contains a tree with leaves. How do we know whether the CNN is using bird-related pixels, as opposed to some other features such as the tree or leaves in the image? This actually happens more often than you think and you should be especially suspicious if you have a small training set. Saliency maps was first introduced in the paper: Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps The idea is pretty simple. We compute the gradient of output category with respect to input image. This should tell us how output category value changes with respect to a small change in input image pixels. All the positive values in the gradients tell us that a small change to that pixel will increase the output value. Hence, visualizing these gradients, which are the same shape as the image should provide some intuition of attention. The idea behind saliency is pretty simple in hindsight. We compute the gradient of output category with respect to input image. \\frac{\\partial output}{\\partial input} This should tell us how the output value changes with respect to a small change in inputs. We can use these gradients to highlight input regions that cause the most change in the output. Intuitively this should highlight salient image regions that most contribute towards the output.","title":"What is Saliency?"},{"location":"visualizations/saliency/#usage","text":"There are two APIs exposed to visualize saliency. visualize_saliency : This is the general purpose API for visualizing saliency. visualize_saliency_with_losses : This is intended for research use-cases where some custom weighted loss can be used. See examples/ for code examples.","title":"Usage"},{"location":"visualizations/saliency/#scenarios","text":"The API is very general purpose and can be used in a wide variety of scenarios. We will list the most common use-cases below:","title":"Scenarios"},{"location":"visualizations/saliency/#categorical-dense-layer-visualization","text":"By setting layer_idx to final Dense layer, and filter_indices to the desired output category, we can visualize parts of the seed_input that contribute most towards activating the corresponding output nodes, For multi-class classification, filter_indices can point to a single class. For multi-label classifier, simply set the appropriate filter_indices .","title":"Categorical Dense layer visualization"},{"location":"visualizations/saliency/#regression-dense-layer-visualization","text":"For regression outputs, we could visualize attention over input that increases decreases maintains the regressed filter_indices output. For example, consider a self driving model with continuous regression steering output. One could visualize parts of the seed_input that contributes towards increase, decrease or maintenance of predicted output. By default, saliency tells us how to increase the output activations. For the self driving car case, this only tells us parts of the input image that contribute towards steering angle increase. Other use cases can be visualized by using grad_modifier option. As the name suggests, it is used to modify the gradient of losses with respect to inputs. To visualize decrease in output, use grad_modifier='negate' . By default, ActivationMaximization loss yields positive gradients for inputs regions that increase the output. By setting grad_modifier='negate' you can treat negative gradients (which indicate the decrease) as positive and therefore visualize decrease use case. To visualize what contributed to the predicted output, we want to consider gradients that have very low positive or negative values. This can be achieved by performing grads = abs(1 / grads) to magnifies small gradients. Equivalently, you can use grad_modifier='small_values' , which does the same thing. gradient_modifiers are very powerful and show up in other visualization APIs as well. You can see a practical application for this in the self diving car example.","title":"Regression Dense layer visualization"},{"location":"visualizations/saliency/#guided-rectified-saliency","text":"Zieler et al. has the idea of clipping negative gradients in the backprop step. i.e., only propagate positive gradient information that communicates the increase in output. We call this rectified or deconv saliency. Details can be found in the paper: Visualizing and Understanding Convolutional Networks . In guided saliency, the backprop step is modified to only propagate positive gradients for positive activations. For details see the paper: String For Simplicity: The All Convolutional Net . For both these cases, we can use backprop_modifier='relu' and backprop_modifier='guided' respectively. You can also implement your own backprop_modifier to try your crazy research idea :)","title":"Guided / rectified saliency"},{"location":"visualizations/saliency/#conv-filter-saliency","text":"By pointing layer_idx to Conv layer, you can visualize parts of the image that influence the filter. This might help you discover what a filter cares about. Here, filter_indices refers to the index of the Conv filter within the layer.","title":"Conv filter saliency"}]}